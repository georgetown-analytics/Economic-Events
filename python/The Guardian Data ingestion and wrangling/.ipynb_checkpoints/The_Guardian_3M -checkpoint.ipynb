{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to download The Guardian UK data and clean data for text analysis\n",
    "@Jorge de Leon \n",
    "\n",
    "This script allows you to download news articles that match your parameters from the Guardian newspaper, https://www.theguardian.com/us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re   \n",
    "import glob\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd \n",
    "\n",
    "from glob import glob\n",
    "from os import makedirs\n",
    "from textblob import TextBlob\n",
    "from os.path import join, exists\n",
    "from datetime import date, timedelta\n",
    "\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API and news articles requests\n",
    "\n",
    "This section contains the code that will be used to download articles from the Guardian website. \n",
    "the initial variables will be determined as user-defined parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter API and parameters - these parameters can be obtained by playing around with the Guardian API tool:\n",
    "# https://open-platform.theguardian.com/explore/\n",
    "\n",
    "# Set up initial and end date \n",
    "\n",
    "start_date_global = date(2000, 1, 1)\n",
    "end_date_global = date(2020, 5, 17)\n",
    "query = \"3M\"\n",
    "term = ('stock')\n",
    "\n",
    "#Enter API key, endpoint and parameters\n",
    "my_api_key = open(\"..\\\\input files\\\\creds_guardian.txt\").read().strip()\n",
    "api_endpoint = \"http://content.guardianapis.com/search?\"\n",
    "my_params = {\n",
    "    'from-date': '',\n",
    "    'to-date': '',\n",
    "    'show-fields': 'bodyText',\n",
    "    'q': query,\n",
    "    'page-size': 200,\n",
    "    'api-key': my_api_key\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_dir = join('theguardian','3m')\n",
    "makedirs(articles_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# day iteration from here:\n",
    "# http://stackoverflow.com/questions/7274267/print-all-day-dates-between-two-dates\n",
    "start_date = start_date_global\n",
    "end_date = end_date_global\n",
    "dayrange = range((end_date - start_date).days + 1)\n",
    "for daycount in dayrange:\n",
    "    dt = start_date + timedelta(days=daycount)\n",
    "    datestr = dt.strftime('%Y-%m-%d')\n",
    "    fname = join(articles_dir, datestr + '.json')\n",
    "    if not exists(fname):\n",
    "        # then let's download it\n",
    "        print(\"Downloading\", datestr)\n",
    "        all_results = []\n",
    "        my_params['from-date'] = datestr\n",
    "        my_params['to-date'] = datestr\n",
    "        current_page = 1\n",
    "        total_pages = 1\n",
    "        while current_page <= total_pages:\n",
    "            print(\"...page\", current_page)\n",
    "            my_params['page'] = current_page\n",
    "            resp = requests.get(api_endpoint, my_params)\n",
    "            data = resp.json()\n",
    "            all_results.extend(data['response']['results'])\n",
    "            # if there is more than one page\n",
    "            current_page += 1\n",
    "            total_pages = data['response']['pages']\n",
    "\n",
    "        with open(fname, 'w') as f:\n",
    "            print(\"Writing to\", fname)\n",
    "\n",
    "            # re-serialize it for pretty indentation\n",
    "            f.write(json.dumps(all_results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read all json files that will be concatenated\n",
    "test_files = sorted(glob('theguardian/3m/*.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intialize empty list that we will append dataframes to\n",
    "all_files = []\n",
    " \n",
    "#write a for loop that will go through each of the file name through globbing and the end result will be the list \n",
    "#of dataframes\n",
    "for file in test_files:\n",
    "    try:\n",
    "        articles = pd.read_json(file)\n",
    "        all_files.append(articles)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print('Note: filename.csv ws empty. Skipping')\n",
    "        continue #will skip the rest of the bloc and move to next file\n",
    "\n",
    "#create dataframe with data from json files\n",
    "theguardian_rawdata = pd.concat(all_files, axis=0, ignore_index=True)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop empty columns\n",
    "theguardian_rawdata = theguardian_rawdata.iloc[:,0:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show types of media that was downloaded by type\n",
    "theguardian_rawdata['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter only for articles\n",
    "theguardian_rawdata = theguardian_rawdata[theguardian_rawdata['type'].str.match('article',na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove columns that do not contain relevant information for analysis\n",
    "theguardian_dataset = theguardian_rawdata.drop(['apiUrl','id', 'isHosted', 'pillarId', 'pillarName',\n",
    "       'sectionId', 'sectionName', 'type','webTitle', 'webUrl'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify the column webPublicationDate to Date and the fields to string and lower case\n",
    "theguardian_dataset[\"date\"] = pd.to_datetime(theguardian_dataset[\"webPublicationDate\"]).dt.strftime('%Y-%m-%d')\n",
    "theguardian_dataset['fields'] = theguardian_dataset['fields'].astype(str).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the articles from URLS, remove punctuaction and numbers. \n",
    "theguardian_dataset['fields'] = theguardian_dataset['fields'].str.replace('<.*?>','') # remove HTML tags\n",
    "theguardian_dataset['fields'] = theguardian_dataset['fields'].str.replace('[^\\w\\s]','') # remove punc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate sentiment analysis for each article\n",
    "#Using TextBlob obtain polarity\n",
    "theguardian_dataset['sentiment_polarity'] = theguardian_dataset['fields'].apply(lambda row: TextBlob(row).sentiment.polarity)\n",
    "#Using TextBlob obtain subjectivity\n",
    "theguardian_dataset['sentiment_subjectivity'] = theguardian_dataset['fields'].apply(lambda row: TextBlob(row).sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove numbers from text\n",
    "theguardian_dataset['fields'] = theguardian_dataset['fields'].str.replace('\\d+','') # remove numbers\n",
    "\n",
    "#Then I will tokenize each word and remover stop words\n",
    "theguardian_dataset['tokenized_fields'] = theguardian_dataset.apply(lambda row: nltk.word_tokenize(row['fields']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop words\n",
    "stop_words=set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stop words\n",
    "theguardian_dataset['tokenized_fields'] = theguardian_dataset['tokenized_fields'].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count number of words and create a column with the most common 5 words per article\n",
    "from collections import Counter\n",
    "theguardian_dataset['high_recurrence'] = theguardian_dataset['tokenized_fields'].apply(lambda x: [k for k, v in Counter(x).most_common(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a word count for the word \"stock\"\n",
    "theguardian_dataset['word_ocurrence'] = theguardian_dataset['tokenized_fields'].apply(lambda x: [w for w in x if re.search(term, w)])\n",
    "theguardian_dataset['word_count'] = theguardian_dataset['word_ocurrence'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a count of the total number of words\n",
    "theguardian_dataset['total_words'] = theguardian_dataset['tokenized_fields'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new table with average polarity, subjectivity, count of the word \"stock\" per day\n",
    "guardian_microsoft = theguardian_dataset.groupby('date')['sentiment_polarity','sentiment_subjectivity','word_count','total_words'].agg('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a variable for the number of articles per day\n",
    "count_articles = theguardian_dataset\n",
    "count_articles['no_articles'] = count_articles.groupby(['date'])['fields'].transform('count')\n",
    "count_articles = count_articles[[\"date\",\"no_articles\"]]\n",
    "count_articles_df = count_articles.drop_duplicates(subset = \"date\", \n",
    "                     keep = \"first\", inplace=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join tables by date\n",
    "guardian_microsoft = guardian_microsoft.merge(count_articles_df, on='date', how ='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save dataframes into CSV\n",
    "theguardian_dataset.to_csv('theguardian/3m/theguardian_3m_text.csv', encoding='utf-8')\n",
    "guardian_microsoft.to_csv('theguardian/3m/theguardian_3m_data.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
