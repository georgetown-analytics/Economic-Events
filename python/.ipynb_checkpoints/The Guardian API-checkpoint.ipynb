{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download articles from The Guardian UK using API\n",
    "\n",
    "Author: Jorge de Leon \n",
    "\n",
    "This script allows you to download news articles that match your parameters from the Guardian newspaper, https://www.theguardian.com/us.\n",
    "\n",
    "The files are downloaded into JSON files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd \n",
    "\n",
    "from os import makedirs\n",
    "from os.path import join, exists\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create directory for files that will be downloaded as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLES_DIR = join('theguardian', 'articles','microsoft')\n",
    "makedirs(ARTICLES_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select parameters and provide API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up initial and end date \n",
    "#- potentially this section will be specified on a global basis for all the pieces of code that have been produced. \n",
    "\n",
    "START_DATE_GLOBAL = date(2020, 3, 1)\n",
    "END_DATE_GLOBAL = date(2020, 4, 12)\n",
    "\n",
    "#Enter API and parameters - these parameters can be obtained by playing around with the Guardian API tool:\n",
    "# https://open-platform.theguardian.com/explore/\n",
    "\n",
    "MY_API_KEY = open(\"..\\\\input files\\\\creds_guardian.txt\").read().strip()\n",
    "API_ENDPOINT = 'http://content.guardianapis.com/search'\n",
    "my_params = {\n",
    "    'from-date': \"\",\n",
    "    'to-date': \"\",\n",
    "    'show-fields': 'all',\n",
    "    'order-by': \"newest\",\n",
    "    'page-size': 200,\n",
    "    'q': \"%22Microsoft%20Corporation%22\",\n",
    "    'api-key': MY_API_KEY\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request files from the Guardian\n",
    "\n",
    "All files including articles, blogs, etc. that match to the search query defined by my paremeters above on a specific day are downloaded and saved to a json file, i.e. there is a json file per day.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 2020-03-01\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-01.json\n",
      "Downloading 2020-03-02\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-02.json\n",
      "Downloading 2020-03-03\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-03.json\n",
      "Downloading 2020-03-04\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-04.json\n",
      "Downloading 2020-03-05\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-05.json\n",
      "Downloading 2020-03-06\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-06.json\n",
      "Downloading 2020-03-07\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-07.json\n",
      "Downloading 2020-03-08\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-08.json\n",
      "Downloading 2020-03-09\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-09.json\n",
      "Downloading 2020-03-10\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-10.json\n",
      "Downloading 2020-03-11\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-11.json\n",
      "Downloading 2020-03-12\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-12.json\n",
      "Downloading 2020-03-13\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-13.json\n",
      "Downloading 2020-03-14\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-14.json\n",
      "Downloading 2020-03-15\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-15.json\n",
      "Downloading 2020-03-16\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-16.json\n",
      "Downloading 2020-03-17\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-17.json\n",
      "Downloading 2020-03-18\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-18.json\n",
      "Downloading 2020-03-19\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-19.json\n",
      "Downloading 2020-03-20\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-20.json\n",
      "Downloading 2020-03-21\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-21.json\n",
      "Downloading 2020-03-22\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-22.json\n",
      "Downloading 2020-03-23\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-23.json\n",
      "Downloading 2020-03-24\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-24.json\n",
      "Downloading 2020-03-25\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-25.json\n",
      "Downloading 2020-03-26\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-26.json\n",
      "Downloading 2020-03-27\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-27.json\n",
      "Downloading 2020-03-28\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-28.json\n",
      "Downloading 2020-03-29\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-29.json\n",
      "Downloading 2020-03-30\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-30.json\n",
      "Downloading 2020-03-31\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-03-31.json\n",
      "Downloading 2020-04-02\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-04-02.json\n",
      "Downloading 2020-04-05\n",
      "...page 1\n",
      "Writing to theguardian\\articles\\microsoft\\2020-04-05.json\n"
     ]
    }
   ],
   "source": [
    "# day iteration from here:\n",
    "# http://stackoverflow.com/questions/7274267/print-all-day-dates-between-two-dates\n",
    "start_date = START_DATE_GLOBAL\n",
    "end_date = END_DATE_GLOBAL\n",
    "dayrange = range((end_date - start_date).days + 1)\n",
    "for daycount in dayrange:\n",
    "    dt = start_date + timedelta(days=daycount)\n",
    "    datestr = dt.strftime('%Y-%m-%d')\n",
    "    fname = join(ARTICLES_DIR, datestr + '.json')\n",
    "    if not exists(fname):\n",
    "        # then let's download it\n",
    "        print(\"Downloading\", datestr)\n",
    "        all_results = []\n",
    "        my_params['from-date'] = datestr\n",
    "        my_params['to-date'] = datestr\n",
    "        current_page = 1\n",
    "        total_pages = 1\n",
    "        while current_page <= total_pages:\n",
    "            print(\"...page\", current_page)\n",
    "            my_params['page'] = current_page\n",
    "            resp = requests.get(API_ENDPOINT, my_params)\n",
    "            data = resp.json()\n",
    "            all_results.extend(data['response']['results'])\n",
    "            # if there is more than one page\n",
    "            current_page += 1\n",
    "            total_pages = data['response']['pages']\n",
    "\n",
    "        with open(fname, 'w') as f:\n",
    "            print(\"Writing to\", fname)\n",
    "\n",
    "            # re-serialize it for pretty indentation\n",
    "            f.write(json.dumps(all_results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to convert files into CSV files\n",
    "\n",
    "For convenience, this code converts all json files to csv files so we can quickly review the content and correct if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_directory = 'theguardian/articles/microsoft/'\n",
    "\n",
    "for file_name in [file for file in os.listdir(test_directory) if file.endswith('.json')]:\n",
    "  with open(test_directory + file_name) as json_file:\n",
    "    data = pd.read_json(json_file)\n",
    "    data.to_csv(test_directory + file_name + '.csv', index =  None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
