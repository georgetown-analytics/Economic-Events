{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Single Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "import pandas as pd\n",
    "from sec_edgar_downloader import Downloader\n",
    "import re\n",
    "import datetime as dt\n",
    "import yfinance as yf\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from pandasql import sqldf\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Last 200 of All types of SEC Documents for Company (Will Take Long Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = Downloader(\"downloaded\\\\notebook\\\\SEC\")\n",
    "for filing_type in dl.supported_filings:\n",
    "    try:\n",
    "        dl.get(filing_type, ticker, 5)\n",
    "    except:\n",
    "        print(\"An Error Occured in Downloading Process\")\n",
    "    print(\"Finished Downloading SEC Documents of \"+ticker)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Sentiment Word List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df=pd.read_csv(\"input files\\\\sentiment words.csv\")\n",
    "#sentiment_df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scan SEC Documents for Sentiment Words (Will Take a Long Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmaster_sec_df= pd.DataFrame(columns=['ticker', 'filing_type', 'path', \\\n",
    "'string_datetime'])\n",
    "\n",
    "for root, dirs, files in os.walk(\"downloaded\\\\notebook\\\\SEC\", topdown=False):\n",
    "    for name in files:\n",
    "        #print(name)\n",
    "        #print(os.path.join(root, name))\n",
    "        temp_path=os.path.join(root, name)\n",
    "        temp_ticker=temp_path.split('\\\\')[4]\n",
    "        temp_type=temp_path.split('\\\\')[5]\n",
    "        \n",
    "        stemp_df= pd.DataFrame(columns=['ticker', 'filing_type', 'path', \\\n",
    "        'string_datetime'])\n",
    "        \n",
    "        try:\n",
    "            temp_text=open(os.path.join(root, name), \"r\").read(1000000)\n",
    "            temp_text_obj=re.search(r'(<ACCEPTANCE-DATETIME>)(.*$)', temp_text, re.M|re.I)\n",
    "        except:\n",
    "            temp_text=\"\"\n",
    "            temp_text_obj=re.search(r'(<ACCEPTANCE-DATETIME>)(.*$)', temp_text, re.M|re.I)\n",
    "        try:\n",
    "            temp_text_dt=temp_text_obj.group(2)\n",
    "        except:\n",
    "            temp_text_dt=\"\"\n",
    "            print(\"No Datetime Found\")\n",
    "\n",
    "        stemp_df =  stemp_df.append({'ticker': temp_ticker, 'filing_type': temp_type, 'path': temp_path, \\\n",
    "        'string_datetime': temp_text_dt}, ignore_index=True)\n",
    "            \n",
    "        for k in range(len(sentiment_df)):\n",
    "            stemp_df[f\"w_{k}\"]=temp_text.upper().count(sentiment_df['word'][k])\n",
    "            \n",
    "        fmaster_sec_df =  fmaster_sec_df.append(stemp_df, ignore_index=True)\n",
    "\n",
    "print(\"SEC Dataset Created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Positive and Negative Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tosave=fmaster_sec_df\n",
    "fmaster_sec_df=tosave\n",
    "fmaster_sec_df['poswords']=0\n",
    "fmaster_sec_df['negwords']=0\n",
    "for k in range(0, 1637):\n",
    "    fmaster_sec_df['poswords']=fmaster_sec_df['poswords']+fmaster_sec_df[f\"w_{k}\"]\n",
    "    fmaster_sec_df=fmaster_sec_df.drop([f\"w_{k}\"], axis=1)\n",
    "\n",
    "for k in range(1637, 3928):\n",
    "    fmaster_sec_df['negwords']=fmaster_sec_df['negwords']+fmaster_sec_df[f\"w_{k}\"]\n",
    "    fmaster_sec_df=fmaster_sec_df.drop([f\"w_{k}\"], axis=1)\n",
    "\n",
    "fmaster_sec_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Date and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmaster_sec_df['datetime']=0 \n",
    "fmaster_sec_df['date']=0\n",
    "for x in range(len(fmaster_sec_df)):\n",
    "    try:\n",
    "        year=int(fmaster_sec_df['string_datetime'][x][0:4])\n",
    "        month=int(fmaster_sec_df['string_datetime'][x][4:6])\n",
    "        day=int(fmaster_sec_df['string_datetime'][x][6:8])\n",
    "        hr=int(fmaster_sec_df['string_datetime'][x][8:10])\n",
    "        minu=int(fmaster_sec_df['string_datetime'][x][10:12])\n",
    "        sec=int(fmaster_sec_df['string_datetime'][x][12:14])\n",
    "        fmaster_sec_df.iloc[x, 6]=dt.datetime(year, month, day, hr, minu, sec)\n",
    "        fmaster_sec_df.iloc[x, 7]=dt.date(year, month, day)\n",
    "        #print(\"line {}\".format(i))\n",
    "    except:\n",
    "        fmaster_sec_df.iloc[x, 6]=0\n",
    "        fmaster_sec_df.iloc[x, 7]=0\n",
    "        print(\"No Extractable Date Value\")\n",
    "        \n",
    "try:\n",
    "    fmaster_sec_df['date']=pd.to_datetime(fmaster_sec_df['date'])\n",
    "except:\n",
    "    fmaster_sec_df['date']=0 \n",
    "\n",
    "fmaster_sec_df=fmaster_sec_df[fmaster_sec_df['date']!=0]\n",
    "\n",
    "fmaster_sec_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Financial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_finance_df= pd.DataFrame(columns=['date', 'price', 'ticker'])\n",
    "\n",
    "temp_df=yf.download(ticker, start='2010-01-01', end='2020-12-31', progress=False)\n",
    "temp_df['date'] = temp_df.index\n",
    "temp_df['price']=temp_df['Close']\n",
    "temp_df=temp_df[['date', 'price']]\n",
    "temp_df['ticker']=ticker\n",
    "\n",
    "master_finance_df=master_finance_df.append(temp_df)\n",
    "\n",
    "    \n",
    "master_finance_df= master_finance_df.rename(columns={'date': 'datetime'})\n",
    "master_finance_df['date']=master_finance_df['datetime'].dt.date\n",
    "master_finance_df.date=pd.to_datetime(master_finance_df.date)\n",
    "\n",
    "sp500tr_df=pd.read_csv(\"input files\\\\SP500TR.csv\")\n",
    "sp500tr_df.date=pd.to_datetime(sp500tr_df.date)\n",
    "\n",
    "master_finance_df=pd.merge(master_finance_df, sp500tr_df, left_on='date', right_on='date', \\\n",
    "validate='one_to_one')\n",
    "\n",
    "\n",
    "master_finance_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Residual Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll_reg(x, k):\n",
    "    x.sort_values(by=['date'])\n",
    "    temp_df_100=x.iloc[k-99:k+1]\n",
    "\n",
    "    results= sm.OLS(temp_df_100['ln_return_price'], sm.add_constant(temp_df_100[['ln_return_index']])).fit()\n",
    "    temp_df_100['residuals'] = results.resid\n",
    "    rolling_resid_return=temp_df_100.iloc[99, 4]\n",
    "    return rolling_resid_return\n",
    "\n",
    "temp_df=master_finance_df[['price','ticker','date','sp500']]\n",
    "temp_df['ln_return_price']=float(0)\n",
    "temp_df['ln_return_index']=float(0)\n",
    "temp_df['roll_resid']=float(0)\n",
    "temp_df.sort_values(by=['date'])\n",
    "\n",
    "for i in range(1, len(temp_df)):\n",
    "    try:\n",
    "        temp_price_ret=np.log(temp_df.iloc[i][0]/temp_df.iloc[i-1][0])\n",
    "        temp_index_ret=np.log(temp_df.iloc[i][3]/temp_df.iloc[i-1][3])\n",
    "        temp_df.iloc[i,4]=temp_price_ret\n",
    "        temp_df.iloc[i,5]=temp_index_ret\n",
    "    except:\n",
    "        temp_df.iloc[i,4]=0\n",
    "        temp_df.iloc[i,5]=0\n",
    "\n",
    "for p in range(100, len(temp_df)-100):\n",
    "        temp_resid=roll_reg(temp_df, p)\n",
    "        temp_df.iloc[p,6]=temp_resid\n",
    "temp_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge SEC Sentiment Data to Financial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#master_finance_df.dtypes\n",
    "#temp_df.dtypes\n",
    "#fmaster_sec_df.dtypes\n",
    "finance_pre_merge_df=master_finance_df[['date', 'ticker', 'price', 'sp500']]\n",
    "temp_pre_merge_df=temp_df[['date', 'ticker', 'ln_return_price', 'ln_return_index', 'roll_resid']]\n",
    "unique_sec_df=fmaster_sec_df.drop_duplicates(subset=[\"date\",\"ticker\"], keep=\"first\")\n",
    "\n",
    "analysis_df1=pd.merge(finance_pre_merge_df, temp_pre_merge_df, how='inner', on=['date', 'ticker'], validate='one_to_one')\n",
    "analysis_df2=pd.merge(analysis_df1, unique_sec_df, how='inner', on=['date', 'ticker'], validate='one_to_one')\n",
    "analysis_df2.head(20)\n",
    "#pre_merge_sec_df.dtypes\n",
    "#pre_merge_sec_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEC Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results=sm.OLS(analysis_df2['roll_resid'], sm.add_constant(analysis_df2[['poswords','negwords']])).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df2['predicted'] = results.fittedvalues\n",
    "analysis_df2.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
